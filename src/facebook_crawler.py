# -*- coding: utf-8 -*-
import os
import time
from datetime import datetime, timedelta
from tqdm import tqdm
import csv
from seleniumwire import webdriver
from fb_graphql_scraper.facebook_graphql_scraper import FacebookGraphqlScraper

# =======================
# âš™ï¸ è¨­å®šå€ï¼šä½¿ç”¨è€…éœ€ä¿®æ”¹
# =======================
DRIVER_PATH = "/path/to/chromedriver"  # â† ä½ çš„ ChromeDriver è·¯å¾‘
FB_USERNAME = None  # â† è‹¥éœ€ç™»å…¥å¸³è™Ÿå‰‡å¡«å…¥å¸³è™Ÿ
FB_PASSWORD = None  # â† è‹¥éœ€ç™»å…¥å¸³è™Ÿå‰‡å¡«å…¥å¯†ç¢¼
TARGET_USER = "nintendo"  # â† ç›®æ¨™ç²‰å°ˆ ID æˆ–ç”¨æˆ¶å
KEYWORDS = ["å¹´å¤œé£¯", "åœçˆ", "æ˜¥ç¯€"]  # â† é—œéµå­—åˆ—è¡¨
MAX_POSTS = 100  # â† è¦çˆ¬çš„è²¼æ–‡æ•¸é‡
DAYS_LIMIT = 30  # â† å¹¾å¤©å…§çš„è²¼æ–‡

# =======================
# ğŸ“ è³‡æ–™å¤¾åˆå§‹åŒ–
# =======================
POST_DIR = "data/facebook/post"
COMMENT_DIR = "data/facebook/comment"
CSV_PATH = "data/instagram/instagram.csv"
os.makedirs(POST_DIR, exist_ok=True)
os.makedirs(COMMENT_DIR, exist_ok=True)
os.makedirs("data/instagram", exist_ok=True)

# =======================
# ğŸš€ å•Ÿå‹• Chrome Driver
# =======================
driver = webdriver.Chrome(executable_path=DRIVER_PATH)
scraper = FacebookGraphqlScraper(driver_path=DRIVER_PATH)

# =======================
# ğŸ•’ çˆ¬å–è²¼æ–‡è³‡æ–™
# =======================
print(f"[Instagram] é—œéµå­—ï¼š{'ã€'.join(KEYWORDS)}")

try:
    results = scraper.get_user_posts(
        fb_username_or_userid=TARGET_USER,
        loop_times=MAX_POSTS,
        username_or_userid=FB_USERNAME,
        pwd=FB_PASSWORD,
        days_limit=DAYS_LIMIT,
        display_progress=False
    )
except Exception as e:
    print(f"âŒ çˆ¬å–éŒ¯èª¤ï¼š{e}")
    driver.quit()
    exit(1)

# =======================
# ğŸ“¦ éæ¿¾ä¸¦å„²å­˜è³‡æ–™
# =======================
filtered_posts = []
progress = tqdm(results[:MAX_POSTS], desc=f"è™•ç†é—œéµå­—ï¼š{KEYWORDS[0]}", ncols=100)

for idx, post in enumerate(progress, start=1):
    try:
        text = post.get("text", "")
        if not any(kw in text for kw in KEYWORDS):
            continue

        post_id = post.get("post_id") or post.get("id") or f"noid_{idx}"
        title = text.strip().split("\n")[0][:50]
        created_time = datetime.fromtimestamp(post.get("time", time.time())).isoformat()
        like_count = post.get("likes", 0)
        comment_count = len(post.get("comments", []))
        reach = post.get("share_count", 0)

        # å„²å­˜è²¼æ–‡
        with open(f"{POST_DIR}/{post_id}.txt", "w", encoding="utf-8") as f:
            f.write(text)

        # å„²å­˜ç•™è¨€
        for comment in post.get("comments", []):
            comment_id = comment.get("comment_id") or comment.get("id") or "nocid"
            comment_text = comment.get("text", "")
            fname = f"post_{post_id}_comment_{comment_id}.txt"
            with open(f"{COMMENT_DIR}/{fname}", "w", encoding="utf-8") as f:
                f.write(comment_text)

        # æ”¶é›†è³‡æ–™ä¾› CSV ä½¿ç”¨
        filtered_posts.append({
            "id": post_id,
            "platform": "instagram",
            "date": created_time,
            "title": title,
            "like_count": like_count,
            "comment_count": comment_count,
            "reach": reach,
            "keywords": ",".join([kw for kw in KEYWORDS if kw in text])
        })

        print(f"\nâœ…å·²è™•ç†ç¬¬ {idx}/{MAX_POSTS} ç¯‡ï¼š{title}")

    except Exception as e:
        print(f"âš ï¸ ç™¼ç”ŸéŒ¯èª¤ï¼Œç•¥éæ­¤ç¯‡ï¼š{e}")
        continue

# =======================
# ğŸ§¾ å„²å­˜ CSV çµ±æ•´è³‡æ–™
# =======================
with open(CSV_PATH, "w", newline="", encoding="utf-8") as csvfile:
    fieldnames = ["id", "platform", "date", "title", "like_count", "comment_count", "reach", "keywords"]
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
    writer.writeheader()
    writer.writerows(filtered_posts)

print(f"\n[Instagram] å·²å„²å­˜è‡³ {CSV_PATH}ï¼Œå…± {len(filtered_posts)} ç­†è³‡æ–™")

# =======================
# âœ… é—œé–‰ç€è¦½å™¨
# =======================
driver.quit()
