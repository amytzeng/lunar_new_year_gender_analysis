# coding: utf-8
import os
import csv
import sys
import time
import datetime
from bs4 import BeautifulSoup
from tqdm import tqdm

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

# Instagram çˆ¬èŸ²ä¸»é¡åˆ¥
class InstagramCrawler:
    def __init__(self, username, password, driver_path, keywords):
        self.username = username
        self.password = password
        self.driver_path = driver_path
        self.driver = None
        self.keywords = keywords
        self.post_count = 0
        self.comment_count = 0
        self.results = []

        # å„²å­˜è³‡æ–™å¤¾åˆå§‹åŒ–
        self.base_dir = os.path.join("data", "instagram")
        self.post_dir = os.path.join(self.base_dir, "post")
        self.comment_dir = os.path.join(self.base_dir, "comment")
        os.makedirs(self.post_dir, exist_ok=True)
        os.makedirs(self.comment_dir, exist_ok=True)

    # å•Ÿå‹•ç€è¦½å™¨
    def initialize_driver(self):
        options = webdriver.ChromeOptions()
        self.driver = webdriver.Chrome(service=Service(self.driver_path), options=options)

    # ç™»å…¥ Instagram
    def login(self):
        self.driver.get("https://www.instagram.com/accounts/login/")
        time.sleep(3)
        username_input = self.driver.find_element(By.NAME, "username")
        password_input = self.driver.find_element(By.NAME, "password")
        username_input.send_keys(self.username)
        password_input.send_keys(self.password)
        self.driver.find_element(By.XPATH, '//*[@id="loginForm"]/div/div[3]/button').click()
        time.sleep(5)  # ç­‰å¾…ç™»å…¥

    # å–å¾—è²¼æ–‡å…§å®¹èˆ‡ç•™è¨€
    def scrape_post(self, url, post_id):
        try:
            self.driver.get(url)
            time.sleep(5)
            soup = BeautifulSoup(self.driver.page_source, "html.parser")

            # æ“·å–è²¼æ–‡æ¨™é¡Œ
            try:
                title = soup.find("meta", property="og:title")["content"]
            except:
                title = ""

            # æ“·å–ç•™è¨€å…ƒç´ ï¼ˆéœ€è¦ç­‰å¾…è¼‰å…¥ï¼‰
            self.scroll_comments()
            soup = BeautifulSoup(self.driver.page_source, "html.parser")
            comment_elements = soup.find_all("span", class_=lambda x: x and "x1lliihq" in x)
            comments = [c.text for c in comment_elements]

            # æª¢æŸ¥æ˜¯å¦åŒ…å«é—œéµå­—
            content_all = title + " " + " ".join(comments)
            if not any(k in content_all for k in self.keywords):
                return  # è·³éæ­¤ç¯‡

            # å„²å­˜è²¼æ–‡
            post_path = os.path.join(self.post_dir, f"{post_id}.txt")
            with open(post_path, "w", encoding="utf-8") as f:
                f.write(title)

            # å„²å­˜ç•™è¨€
            saved_comments = 0
            for i, comment in enumerate(comments):
                if any(k in comment for k in self.keywords):
                    comment_id = self.comment_count + 1
                    comment_path = os.path.join(
                        self.comment_dir, f"post_{post_id}_comment_{comment_id}.txt")
                    with open(comment_path, "w", encoding="utf-8") as f:
                        f.write(comment)
                    self.comment_count += 1
                    saved_comments += 1

            # æ”¶é›†è³‡è¨Šè‡³ csv
            self.results.append({
                "id": post_id,
                "platform": "instagram",
                "date": datetime.datetime.now().strftime("%Y-%m-%d"),
                "title": title,
                "like_count": None,
                "comment_count": saved_comments,
                "reach": None,
                "keywords": ",".join([k for k in self.keywords if k in content_all])
            })

            print(f"âœ…å·²è™•ç†ç¬¬ {post_id}/100 ç¯‡ï¼š{title[:20]}...")
            self.post_count += 1
        except Exception as e:
            print(f"âŒ éŒ¯èª¤ç™¼ç”Ÿæ–¼ç¬¬ {post_id} ç¯‡ï¼š{e}")

    # æ¨¡æ“¬æ»¾å‹•è¼‰å…¥æ‰€æœ‰ç•™è¨€
    def scroll_comments(self, max_scrolls=5):
        try:
            scroll_area = WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "div.x5yr21d"))
            )
        except TimeoutException:
            return

        for _ in range(max_scrolls):
            self.driver.execute_script(
                "arguments[0].scrollTop = arguments[0].scrollHeight", scroll_area)
            time.sleep(2)

    # å„²å­˜æœ€çµ‚ csv çµæœ
    def save_csv(self):
        csv_path = os.path.join(self.base_dir, "instagram.csv")
        with open(csv_path, "w", encoding="utf-8", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=[
                "id", "platform", "date", "title",
                "like_count", "comment_count", "reach", "keywords"
            ])
            writer.writeheader()
            writer.writerows(self.results)
        print(f"[Instagram] å·²å„²å­˜è‡³ {csv_path}ï¼Œå…± {len(self.results)} ç­†è³‡æ–™")

    # é—œé–‰ç€è¦½å™¨
    def close(self):
        if self.driver:
            self.driver.quit()

def run_instagram(keywords):
    from getpass import getpass

    # â— å¯è‡ªè¡Œæ”¹ç‚ºå¾è¨­å®šæª”æˆ–ç’°å¢ƒè®Šæ•¸è®€å–
    username = input("è«‹è¼¸å…¥ Instagram å¸³è™Ÿï¼š")
    password = getpass("è«‹è¼¸å…¥ Instagram å¯†ç¢¼ï¼š")  # å®‰å…¨èµ·è¦‹ç”¨ getpass éš±è—è¼¸å…¥
    driver_path = "/usr/local/bin/chromedriver"  # ä¿®æ”¹ç‚ºä½ æœ¬æ©Ÿè·¯å¾‘

    # è®€ç¶²å€åˆ—è¡¨
    urls_path = os.path.join(os.path.dirname(__file__), "../urls.txt")
    with open(urls_path, "r") as f:
        urls = [line.strip() for line in f if line.strip()]

    crawler = InstagramCrawler(username, password, driver_path, keywords)
    crawler.initialize_driver()
    crawler.login()

    for i, url in enumerate(urls[:100]):
        crawler.scrape_post(url, i + 1)

    crawler.save_csv()
    crawler.close()


# ä¸»ç¨‹å¼
if __name__ == "__main__":
    # Instagram å¸³è™Ÿç™»å…¥è³‡è¨Š
    INSTAGRAM_USERNAME = "your_username"   # ğŸ‘ˆ åœ¨é€™è£¡å¡«å¸³è™Ÿ
    INSTAGRAM_PASSWORD = "your_password"   # ğŸ‘ˆ åœ¨é€™è£¡å¡«å¯†ç¢¼
    CHROME_DRIVER_PATH = "/usr/local/bin/chromedriver"  # ğŸ‘ˆ ä¾ç’°å¢ƒèª¿æ•´è·¯å¾‘

    keywords = ["å›å¨˜å®¶", "ç…®å¹´å¤œé£¯", "å©†åª³é—œä¿‚"]

    # è®€å–ç¶²å€
    with open("urls.txt", "r") as f:
        urls = [line.strip() for line in f if line.strip()]

    crawler = InstagramCrawler(
        INSTAGRAM_USERNAME,
        INSTAGRAM_PASSWORD,
        CHROME_DRIVER_PATH,
        keywords
    )

    crawler.initialize_driver()
    crawler.login()

    # é¡¯ç¤º tqdm é€²åº¦æ¢
    print(f"[Instagram] é—œéµå­—ï¼š{'ã€'.join(keywords)}")
    for i, url in enumerate(tqdm(urls[:100], desc=f"è™•ç†é—œéµå­—ï¼š{'ã€'.join(keywords)}")):
        crawler.scrape_post(url, i + 1)  # post_id å¾ 1 é–‹å§‹

    crawler.save_csv()
    crawler.close()
